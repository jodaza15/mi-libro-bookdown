[["modelado.html", "3 MODELADO 3.1 Modelo de Holt Winters 3.2 EXpanding windows Holt winter 3.3 Metodolog√≠a Box-Jenkins 3.4 Modelo Arima 3.5 Expanding window Arima 3.6 Regresi√≥n de una serie de tiempo y Algoritmo Facebook¬¥s Prophet 3.7 Red Neuronal Elman 3.8 Red Neuronal Elman con Expanding window 3.9 Red neuronal Jordan 3.10 R Markdown 3.11 Including Plots", " 3 MODELADO 3.1 Modelo de Holt Winters La metodolog√≠a de Holt-Winters, tambi√©n conocida como suavizamiento exponencial triple, es una t√©cnica ampliamente utilizada en el an√°lisis de series temporales para realizar pron√≥sticos que presentan patrones de tendencia y estacionalidad. Esta metodolog√≠a extiende el suavizamiento exponencial simple incorporando componentes adicionales que permiten capturar din√°micamente la evoluci√≥n de la tendencia y la estacionalidad a lo largo del tiempo. Holt-Winters se presenta en dos variantes principales: aditiva y multiplicativa, dependiendo de la naturaleza del componente estacional. Es particularmente √∫til en contextos donde los datos muestran fluctuaciones regulares en intervalos espec√≠ficos (como d√≠as, semanas o meses), y permite generar predicciones a corto y mediano plazo con un alto grado de precisi√≥n. Su implementaci√≥n pr√°ctica ha demostrado ser eficaz en √°reas como la econom√≠a, la meteorolog√≠a, la gesti√≥n de inventarios y el consumo de recursos, como agua o energ√≠a(Hurtado Garz√≥n 2013). Teniendo en cuenta la fase de preprocesamiento, se observa que la serie transformada por el logaritmo (es decir no se uso la serie diferenciada que es estacionaria) presenta una estacionalidad visible, as√≠ como una tendencia definida, caracterizada por un comportamiento que decrece, se estabiliza y vuelve a decrecer. Esto indica que cumple con los criterios visuales necesarios para aplicar la metodolog√≠a en cuesti√≥n. Adem√°s, el patr√≥n estacional parece ser claro y repetitivo, lo cual sugiere la presencia de una estacionalidad aditiva. No obstante, al implementar el modelo, es importante considerar los residuos, ya que se identifican picos que podr√≠an estar asociados a eventos at√≠picos o posibles errores de medici√≥n. Cabe resaltar que esta metodolog√≠a no requiere que la serie sea estacionaria. En cambio, se enfoca en identificar una tendencia y una estacionalidad bien definidas, ya que el pron√≥stico se basa en estos dos componentes junto con la media de la serie. A continuaci√≥n, se procede a aplicar el m√©todo de Holt Winter a los precios de cierre diarios del Bitcoin, dentro de la aplicaci√≥n de este modelo se asume una estacionalidad aditiva. #install.packages(&quot;quantmod&quot;) library(quantmod) ## Loading required package: xts ## Loading required package: zoo ## ## Attaching package: &#39;zoo&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## as.Date, as.Date.numeric ## Loading required package: TTR ## Registered S3 method overwritten by &#39;quantmod&#39;: ## method from ## as.zoo.data.frame zoo library(zoo) library(forecast) library(ggplot2) `{r cargar-librerias, message=FALSE, warning=FALSE} library(quantmod) # &lt;- Esta l√≠nea es clave library(zoo) # (si vas a usar as.zoo) # Descargar datos de Yahoo Finance getSymbols(&quot;BTC-USD&quot;, src = &quot;yahoo&quot;, from = &quot;2018-01-01&quot;, to = &quot;2025-04-27&quot;,#Sys.Date(), periodicity = &quot;daily&quot;) ## [1] &quot;BTC-USD&quot; # Convertir el objeto descargado a &#39;zoo&#39; btc_zoo &lt;- as.zoo(`BTC-USD`) # Ver las primeras filas tail(btc_zoo) ## BTC-USD.Open BTC-USD.High BTC-USD.Low BTC-USD.Close BTC-USD.Volume ## 2025-04-22 87521.88 93817.38 87084.53 93441.89 55899038456 ## 2025-04-23 93427.59 94535.73 91962.96 93699.11 41719568821 ## 2025-04-24 93692.40 94016.20 91696.71 93943.80 31483175315 ## 2025-04-25 93954.25 95768.39 92898.59 94720.50 40915232364 ## 2025-04-26 94714.65 95251.36 93927.25 94646.93 17612825123 ## 2025-04-27 94660.91 95301.20 93665.40 93754.84 18090367764 ## BTC-USD.Adjusted ## 2025-04-22 93441.89 ## 2025-04-23 93699.11 ## 2025-04-24 93943.80 ## 2025-04-25 94720.50 ## 2025-04-26 94646.93 ## 2025-04-27 93754.84 library(zoo) btc_close &lt;- as.zoo(btc_zoo$`BTC-USD.Close`) btc_ts &lt;- ts(coredata(btc_close), frequency = 365, start = c(2018, 1)) se procede a realizar la divisi√≥n de datos de entrenamiento y de test # Porcentaje de divisi√≥n train_ratio &lt;- 0.9 n &lt;- length(btc_ts) n_train &lt;- floor(n * train_ratio) n_test &lt;- n - n_train # Crear subconjuntos de entrenamiento y prueba btc_train &lt;- window(btc_ts, end = time(btc_ts)[n_train]) btc_test &lt;- window(btc_ts, start = time(btc_ts)[n_train + 1]) a continuaci√≥n se procede a entrenar el modelo. modelo_hw &lt;- HoltWinters(btc_train) summary(modelo_hw) ## Length Class Mode ## fitted 8164 mts numeric ## x 2406 ts numeric ## alpha 1 -none- numeric ## beta 1 -none- numeric ## gamma 1 -none- numeric ## coefficients 367 -none- numeric ## seasonal 1 -none- character ## SSE 1 -none- numeric ## call 2 -none- call Como siguiente paso se procede a realizar la predicci√≥n sobre el horizonte de prueba # Pron√≥stico sobre los datos de test forecast_hw &lt;- forecast(modelo_hw, h = n_test) autoplot(forecast_hw) + ggtitle(&quot;Pron√≥stico de BTC/USD con Holt-Winters&quot;) + ylab(&quot;Precio de Cierre&quot;) + xlab(&quot;Fecha&quot;) # Graficar plot(forecast_hw, main = &quot;Holt-Winters - Predicci√≥n vs Real&quot;) lines(btc_test, col = &quot;red&quot;, lwd = 2) # Observado en rojo legend(&quot;topleft&quot;, legend = c(&quot;Predicci√≥n&quot;, &quot;Real&quot;), col = c(&quot;blue&quot;, &quot;red&quot;), lty = 1) library(Metrics) ## ## Attaching package: &#39;Metrics&#39; ## The following object is masked from &#39;package:forecast&#39;: ## ## accuracy # Extraer valores predicted &lt;- forecast_hw$mean actual &lt;- btc_test # M√©tricas rmse_val &lt;- rmse(actual, predicted) mae_val &lt;- mae(actual, predicted) mape_val &lt;- mape(actual, predicted) cat(&quot;üìä RMSE:&quot;, rmse_val, &quot;\\n&quot;) ## üìä RMSE: 27326.58 cat(&quot;üìä MAE :&quot;, mae_val, &quot;\\n&quot;) ## üìä MAE : 22457.13 cat(&quot;üìä MAPE:&quot;, round(mape_val * 100, 2), &quot;%\\n&quot;) ## üìä MAPE: 24.56 % 3.2 EXpanding windows Holt winter expanding_holt_winters &lt;- function(serie_ts, initial_train_ratio = 0.8) { library(forecast) library(Metrics) n &lt;- length(serie_ts) n_train &lt;- floor(n * initial_train_ratio) errores &lt;- c() reales &lt;- c() predichos &lt;- c() for (i in n_train:(n - 1)) { # Expanding window hasta el tiempo i ts_train &lt;- window(serie_ts, end = time(serie_ts)[i]) # Ajustar modelo Holt-Winters modelo &lt;- HoltWinters(ts_train) # Predecir 1 paso adelante pred &lt;- forecast(modelo, h = 1)$mean[1] # Valor real del siguiente punto real &lt;- serie_ts[i + 1] # Guardar valores errores &lt;- c(errores, real - pred) reales &lt;- c(reales, real) predichos &lt;- c(predichos, pred) } # Evaluar m√©tricas rmse_val &lt;- rmse(reales, predichos) mae_val &lt;- mae(reales, predichos) mape_val &lt;- mape(reales, predichos) cat(&quot;üìä Expanding Window - Holt-Winters\\n&quot;) cat(&quot;RMSE:&quot;, round(rmse_val, 2), &quot;\\n&quot;) cat(&quot;MAE :&quot;, round(mae_val, 2), &quot;\\n&quot;) cat(&quot;MAPE:&quot;, round(mape_val * 100, 2), &quot;%\\n&quot;) # Gr√°fico comparativo ts_reales &lt;- ts(reales, start = time(serie_ts)[n_train + 1], frequency = frequency(serie_ts)) ts_predichos &lt;- ts(predichos, start = time(serie_ts)[n_train + 1], frequency = frequency(serie_ts)) plot(ts_reales, type = &quot;l&quot;, col = &quot;red&quot;, lwd = 2, main = &quot;Expanding Window: Real vs Predicci√≥n (Holt-Winters)&quot;, ylab = &quot;Precio BTC&quot;) lines(ts_predichos, col = &quot;blue&quot;, lwd = 2) legend(&quot;topleft&quot;, legend = c(&quot;Real&quot;, &quot;Predicci√≥n&quot;), col = c(&quot;red&quot;, &quot;blue&quot;), lty = 1) } resultado_hw&lt;-expanding_holt_winters(btc_ts, initial_train_ratio = 0.9) ## Warning in HoltWinters(ts_train): optimization difficulties: ERROR: ## ABNORMAL_TERMINATION_IN_LNSRCH ## Warning in HoltWinters(ts_train): optimization difficulties: ERROR: ## ABNORMAL_TERMINATION_IN_LNSRCH ## Warning in HoltWinters(ts_train): optimization difficulties: ERROR: ## ABNORMAL_TERMINATION_IN_LNSRCH ## Warning in HoltWinters(ts_train): optimization difficulties: ERROR: ## ABNORMAL_TERMINATION_IN_LNSRCH ## Warning in HoltWinters(ts_train): optimization difficulties: ERROR: ## ABNORMAL_TERMINATION_IN_LNSRCH ## Warning in HoltWinters(ts_train): optimization difficulties: ERROR: ## ABNORMAL_TERMINATION_IN_LNSRCH ## Warning in HoltWinters(ts_train): optimization difficulties: ERROR: ## ABNORMAL_TERMINATION_IN_LNSRCH ## Warning in HoltWinters(ts_train): optimization difficulties: ERROR: ## ABNORMAL_TERMINATION_IN_LNSRCH ## Warning in HoltWinters(ts_train): optimization difficulties: ERROR: ## ABNORMAL_TERMINATION_IN_LNSRCH ## Warning in HoltWinters(ts_train): optimization difficulties: ERROR: ## ABNORMAL_TERMINATION_IN_LNSRCH ## Warning in HoltWinters(ts_train): optimization difficulties: ERROR: ## ABNORMAL_TERMINATION_IN_LNSRCH ## Warning in HoltWinters(ts_train): optimization difficulties: ERROR: ## ABNORMAL_TERMINATION_IN_LNSRCH ## Warning in HoltWinters(ts_train): optimization difficulties: ERROR: ## ABNORMAL_TERMINATION_IN_LNSRCH ## Warning in HoltWinters(ts_train): optimization difficulties: ERROR: ## ABNORMAL_TERMINATION_IN_LNSRCH ## Warning in HoltWinters(ts_train): optimization difficulties: ERROR: ## ABNORMAL_TERMINATION_IN_LNSRCH ## üìä Expanding Window - Holt-Winters ## RMSE: 2255.36 ## MAE : 1608.23 ## MAPE: 1.98 % 3.3 Metodolog√≠a Box-Jenkins La metodolog√≠a Box-Jenkins es un enfoque sistem√°tico que permite identificar, estimar y validar modelos ARMA o ARIMA que se ajusten adecuadamente a una serie de tiempo. Esta metodolog√≠a consta de cuatro etapas principales: Identificaci√≥n del modelo En esta etapa se analiza si la serie es estacionaria. Si no lo es, se aplican transformaciones (como la diferenciaci√≥n) para lograr la estacionariedad. A continuaci√≥n, se identifican los posibles valores de los par√°metros del modelo ARIMA(p, d, q), con base en el an√°lisis gr√°fico y estad√≠stico. Actividades comunes: An√°lisis gr√°fico: para detectar tendencia, estacionalidad o cambios en la media. Prueba de estacionariedad: como la prueba de Dickey-Fuller, que eval√∫a si la serie tiene ra√≠z unitaria. Diferenciaci√≥n: si la serie no es estacionaria, se aplica una o m√°s veces para lograr la estacionariedad y determinar el par√°metro d. Revisi√≥n de ACF y PACF: ACF (autocorrelaci√≥n): ayuda a identificar el orden q (media m√≥vil). PACF (autocorrelaci√≥n parcial): permite sugerir el orden p (autorregresivo). Estimaci√≥n de par√°metros Una vez identificado el modelo, se ajusta a los datos para estimar sus par√°metros. Actividades: Estimar los coeficientes mediante m√©todos como m√°xima verosimilitud. Evaluar la significancia estad√≠stica de los par√°metros. Comparar modelos alternativos utilizando criterios como: AIC (Criterio de Informaci√≥n de Akaike) BIC (Criterio de Informaci√≥n Bayesiano) RMSE (Ra√≠z del error cuadr√°tico medio) Verificaci√≥n del modelo Se valida que los residuos del modelo se comporten como ruido blanco, es decir, que no presenten autocorrelaci√≥n y tengan media cero y varianza constante. Actividades: Analizar los residuos en el tiempo. Revisar los gr√°ficos de ACF y PACF de los residuos. Verificar la normalidad mediante histogramas o pruebas como Shapiro-Wilk. Verificar independencia mediante pruebas como Ljung-Box. Si los supuestos no se cumplen, se debe reconsiderar el modelo y repetir las etapas anteriores. Pron√≥stico Una vez validado el modelo, se procede a generar pron√≥sticos a corto, mediano o largo plazo. Actividades: Generar predicciones con intervalos de confianza. Comparar los pron√≥sticos con datos reales (si est√°n disponibles). Evaluar la precisi√≥n del modelo predictivo. Cabe resaltar que actualmente se dispone de la librer√≠a forecast en R, la cual incluye la funci√≥n auto.arima, que permite seleccionar autom√°ticamente los par√°metros del modelo ARIMA de forma eficiente. Este procedimiento se basa en criterios estad√≠sticos como AIC o BIC para identificar la combinaci√≥n √≥ptima de par√°metros (p,d,q). Gracias a esta automatizaci√≥n, se simplifican varios pasos tradicionales del proceso de modelado, como la inspecci√≥n visual de los gr√°ficos ACF y PACF, la identificaci√≥n manual del grado de diferenciaci√≥n d, y la evaluaci√≥n de m√∫ltiples combinaciones de par√°metros para encontrar el mejor modelo. Por tanto, en esta etapa nos enfocaremos √∫nicamente en la validaci√≥n de los supuestos del modelo sobre los residuos y en la evaluaci√≥n del desempe√±o de los pron√≥sticos (Hurtado Garz√≥n 2013). 3.4 Modelo Arima Los modelos autorregresivos integrados de media m√≥vil (ARIMA, por sus siglas en ingl√©s: AutoRegressive Integrated Moving Average) combinan tres componentes: AR (Autorregresivo): la serie se explica por sus propios valores pasados. I (Integrado): se aplican diferenciaciones a la serie para hacerla estacionaria. MA (Media m√≥vil): se modela el error como una combinaci√≥n lineal de errores pasados. Un modelo ARIMA se denota como ARIMA(p, d, q), donde: p: n√∫mero de t√©rminos autorregresivos (AR). d: n√∫mero de diferenciaciones necesarias para hacer la serie estacionaria. q: n√∫mero de t√©rminos de medias m√≥viles (MA). Estos modelos permiten describir y pronosticar el comportamiento de una serie de tiempo a partir de sus valores y errores pasados (Hurtado Garz√≥n 2013). 3.4.1 Transformaci√≥n serie de tiempo a estacionaria. library(tseries) adf.test(btc_ts,, alternative = c(&quot;stationary&quot;, &quot;explosive&quot;)) ## ## Augmented Dickey-Fuller Test ## ## data: btc_ts ## Dickey-Fuller = -1.9006, Lag order = 13, p-value = 0.6204 ## alternative hypothesis: stationary De acuerdo a la prueba Dickey Fuller se concluye que la serie de tiempo no es estacionaria dado que el Valor P esta por encima 0.05 , tal como se esperaba de acuerdo a su naturaleza de mercado financiero. Por lo tanto a continuaci√≥n se procede a realizar la diferenciaci√≥n de la serie de tiempo. ndiffs(btc_ts) ## [1] 1 ## [1] 1 #nos indica que 1 as√≠ que diferenciamos una vez y la llamamos dif.Indice.ts dif.btc.ts&lt;-diff(btc_ts) #la graficamos plot(dif.btc.ts, main=&quot; &quot;, ylab=&quot;valor&quot;, col=&quot;deepskyblue&quot;, xlab=&quot;A√±os&quot;) title(main=&quot;DIF Precios diarios BTC&quot;) Ahora se procede a confirmar nuevamente con la prueba Dickey Fuller que la serie diferenciada si sea estacionaria. adf.test(dif.btc.ts,, alternative = c(&quot;stationary&quot;, &quot;explosive&quot;)) ## Warning in adf.test(dif.btc.ts, , alternative = c(&quot;stationary&quot;, &quot;explosive&quot;)): ## p-value smaller than printed p-value ## ## Augmented Dickey-Fuller Test ## ## data: dif.btc.ts ## Dickey-Fuller = -12.708, Lag order = 13, p-value = 0.01 ## alternative hypothesis: stationary De acuerdo al resultado de la prueba, se puede afirmar que la serie diferenciada es estacionaria 3.4.2 Funciones de ACF y PACT Continuando con la metodolog√≠a Box-Jenkins, ahora se define y genera las funciones ACF y PACT En an√°lisis de series de tiempo, la ACF (Funci√≥n de Autocorrelaci√≥n) y la PACF (Funci√≥n de Autocorrelaci√≥n Parcial) son herramientas visuales que ayudan a identificar la estructura de dependencia dentro de una serie temporal. La ACF muestra la correlaci√≥n de una serie consigo misma a diferentes rezagos, mientras que la PACF controla la correlaci√≥n de los rezagos anteriores al evaluar la correlaci√≥n en un rezago espec√≠fico. ACF (Funci√≥n de Autocorrelaci√≥n) La ACF grafica la correlaci√≥n entre una variable y sus valores rezagados en diferentes rezagos (diferencia de tiempo entre una observaci√≥n y otra). Ayuda a identificar patrones como estacionalidad, tendencias y persistencia en la serie. Se utiliza para determinar el orden del modelo AR (Autorregresivo) y MA (Promedio M√≥vil). PACF (Funci√≥n de Autocorrelaci√≥n Parcial) La PACF muestra la correlaci√≥n entre una variable y sus valores rezagados, despu√©s de haber eliminado el efecto de los rezagos intermedios. Ayuda a identificar el rezago exacto despu√©s del cual las autocorrelaciones cesan, lo cual es crucial para determinar el orden del modelo AR. Se utiliza para identificar la presencia de patrones estacionales en la serie. Diferencias clave: La ACF considera todas las correlaciones, mientras que la PACF solo considera las correlaciones directas, eliminando las indirectas. La ACF es √∫til para identificar la estructura general de correlaci√≥n, mientras que la PACF ayuda a identificar la estructura de dependencia m√°s precisa. Interpretaci√≥n: Un corte abrupto en el gr√°fico ACF en un rezago espec√≠fico sugiere que un modelo de series temporales con ese n√∫mero de rezagos podr√≠a ser apropiado. Un gr√°fico ACF que decae lentamente puede indicar una tendencia en los datos. Un corte brusco en el gr√°fico PACF puede indicar la presencia de estacionalidad. ACF&lt;-acf(dif.btc.ts) PACF&lt;-pacf(dif.btc.ts) A continuaci√≥n se procede a dividir la serie de tiempo diferenciada para la fasde de entrenamiento y de test # Par√°metros de divisi√≥n train_ratio &lt;- 0.9 n &lt;- length(dif.btc.ts) n_train &lt;- floor(n * train_ratio) n_test &lt;- n - n_train # Dividir en train y test dif_train &lt;- window(dif.btc.ts, end = time(dif.btc.ts)[n_train]) dif_test &lt;- window(dif.btc.ts, start = time(dif.btc.ts)[n_train + 1]) 3.4.3 Modelado Autoarima modelo &lt;- auto.arima(dif_train) modelo ## Series: dif_train ## ARIMA(1,0,0) with zero mean ## ## Coefficients: ## ar1 ## -0.0533 ## s.e. 0.0204 ## ## sigma^2 = 1100170: log likelihood = -20140 ## AIC=40283.99 AICc=40284 BIC=40295.56 pred_arima &lt;- forecast(modelo, h = n_test) d &lt;- ndiffs(btc_ts) last_value &lt;- btc_ts[n_train + d] # valor original antes del primer test predicted &lt;- cumsum(pred_arima$mean) + last_value # Valores reales correspondientes a la serie original actual &lt;- window(btc_ts, start = time(btc_ts)[n_train + d + 1]) summary(actual) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 53949 64326 84623 81836 96457 106146 summary(predicted) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 61614 61615 61615 61615 61615 61625 rmse_val &lt;- rmse(actual, predicted) mae_val &lt;- mae(actual, predicted) mape_val &lt;- mape(actual, predicted) cat(&quot;üìä RMSE:&quot;, rmse_val, &quot;\\n&quot;) ## üìä RMSE: 25727.04 cat(&quot;üìä MAE :&quot;, mae_val, &quot;\\n&quot;) ## üìä MAE : 21253.18 cat(&quot;üìä MAPE:&quot;, round(mape_val * 100, 2), &quot;%\\n&quot;) ## üìä MAPE: 23.31 % predicted_ts &lt;- ts(predicted, start = start(actual), frequency = frequency(btc_ts)) plot(actual, type = &quot;l&quot;, col = &quot;red&quot;, lwd = 2, main = &quot;Predicci√≥n vs Real (ARIMA)&quot;, ylab = &quot;Precio&quot;, ylim = range(c(actual, predicted_ts)) * c(0.9, 1.05)) # m√°s margen abajo lines(predicted_ts, col = &quot;blue&quot;, lwd = 2) 3.5 Expanding window Arima expanding_arima_forecast &lt;- function(serie_ts, initial_train_ratio = 0.9) { library(forecast) library(Metrics) n &lt;- length(serie_ts) n_train &lt;- floor(n * initial_train_ratio) reales &lt;- c() predichos &lt;- c() for (i in n_train:(n - 1)) { # 1. Ventana de entrenamiento hasta el tiempo i ts_train &lt;- window(serie_ts, end = time(serie_ts)[i]) # 2. Ajustar modelo ARIMA autom√°ticamente modelo &lt;- auto.arima(ts_train) # 3. Predecir un paso adelante pred &lt;- forecast(modelo, h = 1)$mean[1] # 4. Obtener valor real siguiente (para evaluaci√≥n) real &lt;- serie_ts[i + 1] # 5. Guardar resultados reales &lt;- c(reales, real) predichos &lt;- c(predichos, pred) } # Convertir a ts para graficar alineado ts_reales &lt;- ts(reales, start = time(serie_ts)[n_train + 1], frequency = frequency(serie_ts)) ts_predichos &lt;- ts(predichos, start = time(serie_ts)[n_train + 1], frequency = frequency(serie_ts)) # Evaluar m√©tricas rmse_val &lt;- rmse(ts_reales, ts_predichos) mae_val &lt;- mae(ts_reales, ts_predichos) mape_val &lt;- mape(ts_reales, ts_predichos) cat(&quot;üìä Expanding Window - ARIMA\\n&quot;) cat(&quot;RMSE:&quot;, round(rmse_val, 2), &quot;\\n&quot;) cat(&quot;MAE :&quot;, round(mae_val, 2), &quot;\\n&quot;) cat(&quot;MAPE:&quot;, round(mape_val * 100, 2), &quot;%\\n&quot;) # Gr√°fico plot(ts_reales, type = &quot;l&quot;, col = &quot;red&quot;, lwd = 2, main = &quot;Expanding Window: Real vs Predicci√≥n (ARIMA)&quot;, ylab = &quot;Precio BTC&quot;) lines(ts_predichos, col = &quot;blue&quot;, lwd = 2) legend(&quot;topleft&quot;, legend = c(&quot;Real&quot;, &quot;Predicci√≥n&quot;), col = c(&quot;red&quot;, &quot;blue&quot;), lty = 1) return(list(real = ts_reales, pred = ts_predichos, rmse = rmse_val, mae = mae_val, mape = mape_val)) } #resultado_arima &lt;- expanding_arima_forecast(btc_ts, initial_train_ratio = 0.9) 3.6 Regresi√≥n de una serie de tiempo y Algoritmo Facebook¬¥s Prophet 3.6.1 Regresi√≥n de una serie El proceso de transformar una serie en una regresi√≥n consiste en usar sus valores pasados(lags) para predecir valores futuros, esto se logra mediante la creaci√≥n de variables yt‚àí1yt‚àí1, yt‚àí2yt‚àí2, en general yt‚àíkyt‚àík para algun k entero y un tiempo t de manera que lo valores ytyt se puedan estimar mediante el modelo: yt=Œ≤0+Œ≤1yt‚àí1+‚Ä¶+Œ≤kyt‚àík+œµt esta es la forma de una regresi√≥n lineal autoregresiva AR(p)(Hamilton 1994).Ademas de esta forma se pueden agregar un conjunto de variables externas XtXt, de forma que el modelo se convierte en:yt=Œ≤0+Œ≤1yt‚àí1+‚Ä¶+Œ≤kyt‚àík+Œ≤k+1Xt+œµtyt=Œ≤0+Œ≤1yt‚àí1+‚Ä¶+Œ≤kyt‚àík+Œ≤k+1Xt+œµt denominado modelo ARX (Ljung 1999) # 1. Crear variable de tiempo tiempo &lt;- 1:length(btc_ts) df_lm &lt;- data.frame( tiempo = tiempo, precio = as.numeric(btc_ts) ) # 2. Particionar por proporci√≥n train_ratio &lt;- 0.8 n &lt;- nrow(df_lm) n_train &lt;- floor(train_ratio * n) # Conjuntos de entrenamiento y prueba train_df &lt;- df_lm[1:n_train, ] test_df &lt;- df_lm[(n_train + 1):n, ] modelo_lm &lt;- lm(precio ~ tiempo, data = train_df) summary(modelo_lm) ## ## Call: ## lm(formula = precio ~ tiempo, data = train_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16646 -8470 -5232 5103 41008 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4677.2955 550.0311 8.504 &lt;2e-16 *** ## tiempo 15.5408 0.4452 34.905 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12710 on 2137 degrees of freedom ## Multiple R-squared: 0.3631, Adjusted R-squared: 0.3628 ## F-statistic: 1218 on 1 and 2137 DF, p-value: &lt; 2.2e-16 pred_test &lt;- predict(modelo_lm, newdata = test_df) actual_test &lt;- test_df$precio rmse_val &lt;- rmse(actual_test, pred_test) mae_val &lt;- mae(actual_test, pred_test) mape_val &lt;- mape(actual_test, pred_test) cat(&quot;üìä RMSE:&quot;, round(rmse_val, 2), &quot;\\n&quot;) ## üìä RMSE: 31857.51 cat(&quot;üìä MAE :&quot;, round(mae_val, 2), &quot;\\n&quot;) ## üìä MAE : 27069.4 cat(&quot;üìä MAPE:&quot;, round(mape_val * 100, 2), &quot;%\\n&quot;) ## üìä MAPE: 35.12 % plot(test_df$tiempo, actual_test, type = &quot;l&quot;, col = &quot;red&quot;, lwd = 2, main = &quot;Regresi√≥n Lineal: Predicci√≥n vs Real&quot;, xlab = &quot;Tiempo&quot;, ylab = &quot;Precio BTC&quot;) lines(test_df$tiempo, pred_test, col = &quot;blue&quot;, lwd = 2) legend(&quot;topleft&quot;, legend = c(&quot;Real&quot;, &quot;Predicci√≥n&quot;), col = c(&quot;red&quot;, &quot;blue&quot;), lty = 1, lwd = 2) ## Expanding Window Arima expanding_linear_forecast &lt;- function(serie_ts, initial_train_ratio = 0.9) { library(Metrics) n &lt;- length(serie_ts) n_train &lt;- floor(n * initial_train_ratio) reales &lt;- c() predichos &lt;- c() for (i in n_train:(n - 1)) { # Construir datos de entrenamiento hasta el tiempo i ts_train &lt;- serie_ts[1:i] tiempo_train &lt;- 1:i df_train &lt;- data.frame( tiempo = tiempo_train, precio = as.numeric(ts_train) ) # Ajustar modelo lineal modelo &lt;- lm(precio ~ tiempo, data = df_train) # Predecir el siguiente punto (tiempo i+1) nuevo &lt;- data.frame(tiempo = i + 1) pred &lt;- predict(modelo, newdata = nuevo) # Valor real en t = i + 1 real &lt;- as.numeric(serie_ts[i + 1]) # Guardar resultados reales &lt;- c(reales, real) predichos &lt;- c(predichos, pred) } # Convertir a ts para alinear ts_reales &lt;- ts(reales, start = time(serie_ts)[n_train + 1], frequency = frequency(serie_ts)) ts_predichos &lt;- ts(predichos, start = time(serie_ts)[n_train + 1], frequency = frequency(serie_ts)) # Evaluar m√©tricas rmse_val &lt;- rmse(ts_reales, ts_predichos) mae_val &lt;- mae(ts_reales, ts_predichos) mape_val &lt;- mape(ts_reales, ts_predichos) cat(&quot;üìä Expanding Window - Regresi√≥n Lineal\\n&quot;) cat(&quot;RMSE:&quot;, round(rmse_val, 2), &quot;\\n&quot;) cat(&quot;MAE :&quot;, round(mae_val, 2), &quot;\\n&quot;) cat(&quot;MAPE:&quot;, round(mape_val * 100, 2), &quot;%\\n&quot;) # Gr√°fico plot(ts_reales, type = &quot;l&quot;, col = &quot;red&quot;, lwd = 2, main = &quot;Expanding Window: Real vs Predicci√≥n (Regresi√≥n Lineal)&quot;, ylab = &quot;Precio BTC&quot;) lines(ts_predichos, col = &quot;blue&quot;, lwd = 2) legend(&quot;topleft&quot;, legend = c(&quot;Real&quot;, &quot;Predicci√≥n&quot;), col = c(&quot;red&quot;, &quot;blue&quot;), lty = 1) return(list(real = ts_reales, pred = ts_predichos, rmse = rmse_val, mae = mae_val, mape = mape_val)) } resultado_lm &lt;- expanding_linear_forecast(btc_ts, initial_train_ratio = 0.9) ## üìä Expanding Window - Regresi√≥n Lineal ## RMSE: 28586.75 ## MAE : 25435.98 ## MAPE: 29.17 % 3.6.2 Algoritmo Facebook¬¥s Prophet Prophet es un algoritmo desarrollado por el equipo de investigaci√≥n de facebook para el pronostico de series de tiempo, su finalidad es la robustez frente a datos faltantes, cambios en la tendencia y m√∫ltiples estacionalidades. El modelo Prophet no es una regresi√≥n como tal, mas bien es una descomposici√≥n de forma aditiva, ya que se asume una descomposici√≥n de la serie de la siguiente forma: y(t)=g(t)+s(t)+h(t)+œµty(t)=g(t)+s(t)+h(t)+œµt donde: g(t):tendencia s(t):estacionalidad g(t):efecto de dias festivos œµt:error(ruido blanco) dado que este metodo solo busca pronosticar valores futuros, no es necesario validar supuestos de normalidad en los errores, sin embargo si se debe verificar como una buena practica, que estos no est√©n correlacionados y que tengan media cero y varianza constante(Facebook Core Data Science Team 2023). library(prophet) ## Loading required package: Rcpp ## Loading required package: rlang ## ## Attaching package: &#39;rlang&#39; ## The following object is masked from &#39;package:Metrics&#39;: ## ## ll start_date &lt;- as.Date(&quot;2018-01-01&quot;) btc_df &lt;- data.frame( ds = seq(start_date, by = &quot;day&quot;, length.out = length(btc_ts)), y = as.numeric(btc_ts) ) n &lt;- nrow(btc_df) n_train &lt;- floor(n * 0.8) df_train &lt;- btc_df[1:n_train, ] df_test &lt;- btc_df[(n_train + 1):n, ] modelo &lt;- prophet(df_train) ## Disabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this. # 4. Crear dataframe futuro para predecir el mismo horizonte de test future &lt;- make_future_dataframe(modelo, periods = n - n_train) # 5. Predecir forecast &lt;- predict(modelo, future) # 6. Extraer solo predicciones del per√≠odo de prueba pred_test &lt;- forecast[(n_train + 1):n, ] rmse_val &lt;- rmse(df_test$y, pred_test$yhat) mae_val &lt;- mae(df_test$y, pred_test$yhat) mape_val &lt;- mape(df_test$y, pred_test$yhat) cat(&quot;üìä Prophet con divisi√≥n 80/20:\\n&quot;) ## üìä Prophet con divisi√≥n 80/20: cat(&quot;RMSE:&quot;, round(rmse_val, 2), &quot;\\n&quot;) ## RMSE: 33526.99 cat(&quot;MAE :&quot;, round(mae_val, 2), &quot;\\n&quot;) ## MAE : 29892.62 cat(&quot;MAPE:&quot;, round(mape_val * 100, 2), &quot;%\\n&quot;) ## MAPE: 40.53 % # 8. Gr√°fico plot(df_test$ds, df_test$y, type = &quot;l&quot;, col = &quot;red&quot;, lwd = 2, main = &quot;Predicci√≥n Prophet vs Real (80/20)&quot;, ylab = &quot;Precio BTC&quot;, xlab = &quot;Fecha&quot;) lines(pred_test$ds, pred_test$yhat, col = &quot;blue&quot;, lwd = 2) legend(&quot;topleft&quot;, legend = c(&quot;Real&quot;, &quot;Predicci√≥n&quot;), col = c(&quot;red&quot;, &quot;blue&quot;), lty = 1) ## Expading Window Prophet expanding_prophet_forecast &lt;- function(btc_ts, initial_train_ratio = 0.9) { library(prophet) library(Metrics) # 1. Preparar datos con fecha y valor btc_df &lt;- data.frame( ds = seq.Date(from = as.Date(&quot;2018-01-01&quot;), by = &quot;day&quot;, length.out = length(btc_ts)), y = as.numeric(btc_ts) ) n &lt;- nrow(btc_df) n_train &lt;- floor(n * initial_train_ratio) reales &lt;- c() predichos &lt;- c() fechas &lt;- c() for (i in n_train:(n - 1)) { # Subconjunto hasta t = i df_train &lt;- btc_df[1:i, ] # Entrenar modelo Prophet modelo &lt;- prophet(df_train, verbose = FALSE, daily.seasonality = FALSE) # Crear data.frame para predecir t = i + 1 fecha_pred &lt;- btc_df$ds[i + 1] future &lt;- data.frame(ds = fecha_pred) # Predecir forecast &lt;- predict(modelo, future) # Guardar resultados reales &lt;- c(reales, btc_df$y[i + 1]) predichos &lt;- c(predichos, forecast$yhat[1]) fechas &lt;- c(fechas, as.character(fecha_pred)) } # Convertir a serie de tiempo ts_reales &lt;- ts(reales, start = n_train + 1, frequency = 1) ts_predichos &lt;- ts(predichos, start = n_train + 1, frequency = 1) # M√©tricas rmse_val &lt;- rmse(ts_reales, ts_predichos) mae_val &lt;- mae(ts_reales, ts_predichos) mape_val &lt;- mape(ts_reales, ts_predichos) cat(&quot;üìä Expanding Window - Prophet\\n&quot;) cat(&quot;RMSE:&quot;, round(rmse_val, 2), &quot;\\n&quot;) cat(&quot;MAE :&quot;, round(mae_val, 2), &quot;\\n&quot;) cat(&quot;MAPE:&quot;, round(mape_val * 100, 2), &quot;%\\n&quot;) # Gr√°fico fechas_ts &lt;- as.Date(fechas) plot(fechas_ts, ts_reales, type = &quot;l&quot;, col = &quot;red&quot;, lwd = 2, main = &quot;Expanding Window: Real vs Predicci√≥n (Prophet)&quot;, ylab = &quot;Precio BTC&quot;, xlab = &quot;Fecha&quot;) lines(fechas_ts, ts_predichos, col = &quot;blue&quot;, lwd = 2) legend(&quot;topleft&quot;, legend = c(&quot;Real&quot;, &quot;Predicci√≥n&quot;), col = c(&quot;red&quot;, &quot;blue&quot;), lty = 1) return(data.frame( fecha = fechas_ts, real = ts_reales, pred = ts_predichos )) } resultado_prophet &lt;- expanding_prophet_forecast(btc_ts, initial_train_ratio = 0.9) ## üìä Expanding Window - Prophet ## RMSE: 11777.56 ## MAE : 10131.19 ## MAPE: 12.34 % 3.7 Red Neuronal Elman install.packages(&quot;RSNNS&quot;) ## Installing package into &#39;/cloud/lib/x86_64-pc-linux-gnu-library/4.4&#39; ## (as &#39;lib&#39; is unspecified) library(RSNNS) # Normalizar serie btc_norm &lt;- (btc_ts - min(btc_ts)) / (max(btc_ts) - min(btc_ts)) # Par√°metros n_lags &lt;- 7 n &lt;- length(btc_norm) n_train &lt;- floor(n * 0.9) # Crear matriz de entrada y salida crear_dataset &lt;- function(serie, n_lags) { X &lt;- embed(serie, n_lags + 1) y &lt;- X[, 1] X &lt;- X[, -1] return(list(X = X, y = y)) } datos &lt;- crear_dataset(btc_norm, n_lags) X &lt;- datos$X y &lt;- datos$y # Divisi√≥n train/test X_train &lt;- X[1:(n_train - n_lags), ] y_train &lt;- y[1:(n_train - n_lags)] X_test &lt;- X[(n_train - n_lags + 1):(n - n_lags), ] y_test &lt;- y[(n_train - n_lags + 1):(n - n_lags)] # Ajustar red Elman modelo_elman &lt;- elman(X_train, y_train, size = 10, learnFuncParams = c(0.1), maxit = 100, linOut = TRUE) # Predicci√≥n pred_norm &lt;- predict(modelo_elman, X_test) # Desnormalizar min_y &lt;- min(btc_ts) max_y &lt;- max(btc_ts) pred &lt;- pred_norm * (max_y - min_y) + min_y real &lt;- y_test * (max_y - min_y) + min_y library(Metrics) cat(&quot;üìä Elman Neural Net:\\n&quot;) ## üìä Elman Neural Net: cat(&quot;RMSE:&quot;, round(rmse(real, pred), 2), &quot;\\n&quot;) ## RMSE: 15327.92 cat(&quot;MAE :&quot;, round(mae(real, pred), 2), &quot;\\n&quot;) ## MAE : 12516.27 cat(&quot;MAPE:&quot;, round(mape(real, pred) * 100, 2), &quot;%\\n&quot;) ## MAPE: 13.7 % # Gr√°fico plot(real, type = &quot;l&quot;, col = &quot;red&quot;, lwd = 2, ylab = &quot;Precio BTC&quot;, main = &quot;Red Elman&quot;) lines(pred, col = &quot;blue&quot;, lwd = 2) legend(&quot;topleft&quot;, legend = c(&quot;Real&quot;, &quot;Predicho&quot;), col = c(&quot;red&quot;, &quot;blue&quot;), lty = 1) 3.8 Red Neuronal Elman con Expanding window expanding_elman_forecast &lt;- function(btc_ts, n_lags = 5, size = 10, initial_train_ratio = 0.8, maxit = 100) { library(RSNNS) library(Metrics) # Normalizar la serie btc_norm &lt;- (btc_ts - min(btc_ts)) / (max(btc_ts) - min(btc_ts)) min_val &lt;- min(btc_ts) max_val &lt;- max(btc_ts) # Preparar funci√≥n para ventana deslizante crear_dataset &lt;- function(serie, n_lags) { X &lt;- embed(serie, n_lags + 1) y &lt;- X[, 1] X &lt;- X[, -1] return(list(X = X, y = y)) } datos &lt;- crear_dataset(btc_norm, n_lags) X_all &lt;- datos$X y_all &lt;- datos$y total &lt;- nrow(X_all) # Punto inicial para expanding window start &lt;- floor(total * initial_train_ratio) pred &lt;- c() real &lt;- c() for (i in start:(total - 1)) { X_train &lt;- X_all[1:i, ] y_train &lt;- y_all[1:i] X_next &lt;- matrix(X_all[i + 1, ], nrow = 1) # Ajustar red Elman modelo &lt;- elman(X_train, y_train, size = size, learnFuncParams = c(0.1), maxit = maxit, linOut = TRUE) pred_i &lt;- predict(modelo, X_next) pred &lt;- c(pred, pred_i) real &lt;- c(real, y_all[i + 1]) } # Desnormalizar pred_desnorm &lt;- pred * (max_val - min_val) + min_val real_desnorm &lt;- real * (max_val - min_val) + min_val # M√©tricas rmse_val &lt;- rmse(real_desnorm, pred_desnorm) mae_val &lt;- mae(real_desnorm, pred_desnorm) mape_val &lt;- mape(real_desnorm, pred_desnorm) cat(&quot;üìä Expanding Window - Red Elman\\n&quot;) cat(&quot;RMSE:&quot;, round(rmse_val, 2), &quot;\\n&quot;) cat(&quot;MAE :&quot;, round(mae_val, 2), &quot;\\n&quot;) cat(&quot;MAPE:&quot;, round(mape_val * 100, 2), &quot;%\\n&quot;) # Gr√°fico plot(real_desnorm, type = &quot;l&quot;, col = &quot;red&quot;, lwd = 2, ylab = &quot;Precio BTC&quot;, main = &quot;Red Elman - Expanding Window&quot;) lines(pred_desnorm, col = &quot;blue&quot;, lwd = 2) legend(&quot;topleft&quot;, legend = c(&quot;Real&quot;, &quot;Predicho&quot;), col = c(&quot;red&quot;, &quot;blue&quot;), lty = 1) return(data.frame(real = real_desnorm, pred = pred_desnorm)) } resultado_elman &lt;- expanding_elman_forecast(btc_ts, n_lags = 7, size = 10) ## üìä Expanding Window - Red Elman ## RMSE: 2137.92 ## MAE : 1542.39 ## MAPE: 2.24 % 3.9 Red neuronal Jordan expanding_jordan_forecast &lt;- function(btc_ts, n_lags = 7, size = 10, initial_train_ratio = 0.9, maxit = 100) { library(RSNNS) library(Metrics) # 1. Normalizar serie btc_norm &lt;- (btc_ts - min(btc_ts)) / (max(btc_ts) - min(btc_ts)) min_val &lt;- min(btc_ts) max_val &lt;- max(btc_ts) # 2. Crear dataset con ventanas crear_dataset &lt;- function(serie, n_lags) { X &lt;- embed(serie, n_lags + 1) y &lt;- X[, 1] X &lt;- X[, -1] return(list(X = X, y = y)) } datos &lt;- crear_dataset(btc_norm, n_lags) X_all &lt;- datos$X y_all &lt;- datos$y total &lt;- nrow(X_all) # 3. Divisi√≥n inicial start &lt;- floor(total * initial_train_ratio) pred &lt;- c() real &lt;- c() for (i in start:(total - 1)) { X_train &lt;- X_all[1:i, ] y_train &lt;- y_all[1:i] X_next &lt;- matrix(X_all[i + 1, ], nrow = 1) # 4. Ajustar modelo Jordan modelo &lt;- jordan(X_train, y_train, size = size, learnFuncParams = c(0.1), maxit = maxit, linOut = TRUE) pred_i &lt;- predict(modelo, X_next) pred &lt;- c(pred, pred_i) real &lt;- c(real, y_all[i + 1]) } # 5. Desnormalizar pred_desnorm &lt;- pred * (max_val - min_val) + min_val real_desnorm &lt;- real * (max_val - min_val) + min_val # 6. M√©tricas rmse_val &lt;- rmse(real_desnorm, pred_desnorm) mae_val &lt;- mae(real_desnorm, pred_desnorm) mape_val &lt;- mape(real_desnorm, pred_desnorm) cat(&quot;üìä Expanding Window - Red Jordan\\n&quot;) cat(&quot;RMSE:&quot;, round(rmse_val, 2), &quot;\\n&quot;) cat(&quot;MAE :&quot;, round(mae_val, 2), &quot;\\n&quot;) cat(&quot;MAPE:&quot;, round(mape_val * 100, 2), &quot;%\\n&quot;) # 7. Gr√°fico plot(real_desnorm, type = &quot;l&quot;, col = &quot;red&quot;, lwd = 2, ylab = &quot;Precio BTC&quot;, main = &quot;Red Jordan - Expanding Window&quot;) lines(pred_desnorm, col = &quot;blue&quot;, lwd = 2) legend(&quot;topleft&quot;, legend = c(&quot;Real&quot;, &quot;Predicho&quot;), col = c(&quot;red&quot;, &quot;blue&quot;), lty = 1) return(data.frame(real = real_desnorm, pred = pred_desnorm)) } resultado_jordan &lt;- expanding_jordan_forecast(btc_ts, n_lags = 7, size = 10) ## üìä Expanding Window - Red Jordan ## RMSE: 2350.64 ## MAE : 1756.91 ## MAPE: 2.16 % 3.10 R Markdown 3.11 Including Plots "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
